{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d7e4358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from sqlalchemy import create_engine # database connection\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a1d0376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jraldua-veuthey\\AppData\\Local\\Temp\\ipykernel_7820\\968316404.py:11: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_name_csv)\n"
     ]
    }
   ],
   "source": [
    "url = \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2020-01.csv.gz\"\n",
    "file_name_gz = \"yellow_tripdata_2020-01.csv.gz\"\n",
    "file_name_csv = \"yellow_tripdata_2020-01.csv\"\n",
    "\n",
    "urllib.request.urlretrieve(url, file_name_gz)\n",
    "\n",
    "with gzip.open(file_name_gz, 'rb') as f_in:\n",
    "    with open(file_name_csv, 'wb') as f_out:\n",
    "        f_out.write(f_in.read())\n",
    "\n",
    "df = pd.read_csv(file_name_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "339a5c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "791b0846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE yellow_taxi_data (\n",
      "\t\"VendorID\" FLOAT(53), \n",
      "\ttpep_pickup_datetime TEXT, \n",
      "\ttpep_dropoff_datetime TEXT, \n",
      "\tpassenger_count FLOAT(53), \n",
      "\ttrip_distance FLOAT(53), \n",
      "\t\"RatecodeID\" FLOAT(53), \n",
      "\tstore_and_fwd_flag TEXT, \n",
      "\t\"PULocationID\" BIGINT, \n",
      "\t\"DOLocationID\" BIGINT, \n",
      "\tpayment_type FLOAT(53), \n",
      "\tfare_amount FLOAT(53), \n",
      "\textra FLOAT(53), \n",
      "\tmta_tax FLOAT(53), \n",
      "\ttip_amount FLOAT(53), \n",
      "\ttolls_amount FLOAT(53), \n",
      "\timprovement_surcharge FLOAT(53), \n",
      "\ttotal_amount FLOAT(53), \n",
      "\tcongestion_surcharge FLOAT(53)\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pd.io.sql.get_schema(df, name='yellow_taxi_data', con=engine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de7b9007",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iter = pd.read_csv(file_name_csv, iterator=True, chunksize=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ec24c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = next(df_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c72593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8252c284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=0).to_sql(name='yellow_taxi_data', con=engine, if_exists='replace') # this creates the table in the database with only the heders (since n=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98959ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.59 s\n",
      "Wall time: 8.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df.to_sql(name='yellow_taxi_data', con=engine, if_exists='append') # this appends the data to the table\n",
    "# %time is a magic function that measures the time it takes to run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98959ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.59 s\n",
      "Wall time: 8.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%time df.to_sql(name='yellow_taxi_data', con=engine, if_exists='append') # this appends the data to the table\n",
    "# %time is a magic function that measures the time it takes to run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09cd464a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted another chunk, took 9.701 second\n",
      "inserted another chunk, took 9.030 second\n",
      "inserted another chunk, took 8.772 second\n",
      "inserted another chunk, took 9.893 second\n",
      "inserted another chunk, took 9.937 second\n",
      "inserted another chunk, took 10.631 second\n",
      "inserted another chunk, took 10.715 second\n",
      "inserted another chunk, took 12.382 second\n",
      "inserted another chunk, took 11.086 second\n",
      "inserted another chunk, took 9.441 second\n",
      "inserted another chunk, took 9.739 second\n",
      "inserted another chunk, took 8.706 second\n",
      "inserted another chunk, took 9.318 second\n",
      "inserted another chunk, took 9.742 second\n",
      "inserted another chunk, took 10.067 second\n",
      "inserted another chunk, took 10.040 second\n",
      "inserted another chunk, took 21.495 second\n",
      "inserted another chunk, took 13.326 second\n",
      "inserted another chunk, took 14.037 second\n",
      "inserted another chunk, took 12.040 second\n",
      "inserted another chunk, took 11.283 second\n",
      "inserted another chunk, took 10.977 second\n",
      "inserted another chunk, took 9.914 second\n",
      "inserted another chunk, took 12.093 second\n",
      "inserted another chunk, took 9.993 second\n",
      "inserted another chunk, took 9.709 second\n",
      "inserted another chunk, took 10.147 second\n",
      "inserted another chunk, took 14.138 second\n",
      "inserted another chunk, took 9.842 second\n",
      "inserted another chunk, took 9.903 second\n",
      "inserted another chunk, took 9.256 second\n",
      "inserted another chunk, took 9.694 second\n",
      "inserted another chunk, took 8.311 second\n",
      "inserted another chunk, took 8.709 second\n",
      "inserted another chunk, took 9.872 second\n",
      "inserted another chunk, took 12.385 second\n",
      "inserted another chunk, took 10.362 second\n",
      "inserted another chunk, took 8.955 second\n",
      "inserted another chunk, took 8.721 second\n",
      "inserted another chunk, took 8.526 second\n",
      "inserted another chunk, took 9.726 second\n",
      "inserted another chunk, took 10.043 second\n",
      "inserted another chunk, took 10.345 second\n",
      "inserted another chunk, took 11.987 second\n",
      "inserted another chunk, took 9.621 second\n",
      "inserted another chunk, took 9.272 second\n",
      "inserted another chunk, took 9.683 second\n",
      "inserted another chunk, took 9.369 second\n",
      "inserted another chunk, took 9.514 second\n",
      "inserted another chunk, took 8.470 second\n",
      "inserted another chunk, took 8.238 second\n",
      "inserted another chunk, took 8.437 second\n",
      "inserted another chunk, took 8.724 second\n",
      "inserted another chunk, took 8.285 second\n",
      "inserted another chunk, took 10.110 second\n",
      "inserted another chunk, took 11.048 second\n",
      "inserted another chunk, took 10.035 second\n",
      "inserted another chunk, took 10.045 second\n",
      "inserted another chunk, took 10.158 second\n",
      "inserted another chunk, took 10.370 second\n",
      "inserted another chunk, took 11.131 second\n",
      "inserted another chunk, took 9.853 second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jraldua-veuthey\\AppData\\Local\\Temp\\ipykernel_7820\\2113580993.py:4: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = next(df_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted another chunk, took 9.541 second\n",
      "inserted another chunk, took 0.502 second\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m: \n\u001b[0;32m      2\u001b[0m     t_start \u001b[39m=\u001b[39m time()\n\u001b[1;32m----> 4\u001b[0m     df \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(df_iter)\n\u001b[0;32m      6\u001b[0m     df\u001b[39m.\u001b[39mtpep_pickup_datetime \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(df\u001b[39m.\u001b[39mtpep_pickup_datetime)\n\u001b[0;32m      7\u001b[0m     df\u001b[39m.\u001b[39mtpep_dropoff_datetime \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(df\u001b[39m.\u001b[39mtpep_dropoff_datetime)\n",
      "File \u001b[1;32mc:\\Users\\jraldua-veuthey\\.virtualenvs\\data-eng-zc-gcp-aws-Y2KD2JE8\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1698\u001b[0m, in \u001b[0;36mTextFileReader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1696\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[0;32m   1697\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1698\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_chunk()\n\u001b[0;32m   1699\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m   1700\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\jraldua-veuthey\\.virtualenvs\\data-eng-zc-gcp-aws-Y2KD2JE8\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1810\u001b[0m, in \u001b[0;36mTextFileReader.get_chunk\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1808\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n\u001b[0;32m   1809\u001b[0m     size \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnrows \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_currow)\n\u001b[1;32m-> 1810\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nrows\u001b[39m=\u001b[39;49msize)\n",
      "File \u001b[1;32mc:\\Users\\jraldua-veuthey\\.virtualenvs\\data-eng-zc-gcp-aws-Y2KD2JE8\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1771\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1772\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1773\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m     (\n\u001b[0;32m   1775\u001b[0m         index,\n\u001b[0;32m   1776\u001b[0m         columns,\n\u001b[0;32m   1777\u001b[0m         col_dict,\n\u001b[1;32m-> 1778\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1779\u001b[0m         nrows\n\u001b[0;32m   1780\u001b[0m     )\n\u001b[0;32m   1781\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\jraldua-veuthey\\.virtualenvs\\data-eng-zc-gcp-aws-Y2KD2JE8\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 230\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[0;32m    231\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    232\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mc:\\Users\\jraldua-veuthey\\.virtualenvs\\data-eng-zc-gcp-aws-Y2KD2JE8\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:833\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True: \n",
    "    t_start = time()\n",
    "\n",
    "    df = next(df_iter)\n",
    "\n",
    "    df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "    df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n",
    "    \n",
    "    df.to_sql(name='yellow_taxi_data', con=engine, if_exists='append')\n",
    "\n",
    "    t_end = time()\n",
    "\n",
    "    print('inserted another chunk, took %.3f second' % (t_end - t_start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-eng-zc-gcp-aws-Y2KD2JE8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "35dc32dd66dc6a9570e9158145834b007d61225e6c3cc4f50904136dd8e8a457"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
